{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Descriptor functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_onset_count(v_list):\n",
    "    if v_list == [] * len(v_list):\n",
    "        return 0\n",
    "    return (np.array(v_list) > 0).sum() / len(v_list)\n",
    "\n",
    "def v_start(v_list):\n",
    "    if v_onset_count(v_list) == 0:\n",
    "        return 0\n",
    "    s = 0\n",
    "    while v_list[s] == 0:\n",
    "        s += 1\n",
    "    return s / (len(v_list))\n",
    "\n",
    "def v_center(v_list):\n",
    "    if v_onset_count(v_list) == 0:\n",
    "        return 0\n",
    "    return np.mean([i + 1 for i, x in enumerate(v_list) if x > 0]) / len(v_list)\n",
    "\n",
    "def v_syncopation(v_list):\n",
    "    if v_onset_count(v_list) == 0:\n",
    "        return 0\n",
    "    v_list = np.array(v_list)\n",
    "    mw = [5, 1, 2, 1, 3, 1, 2, 1, 4, 1, 2, 1, 3, 1, 2, 1]  # metrical weights\n",
    "    n = len(v_list)\n",
    "    sync_list = np.zeros(n)\n",
    "    for i, x in enumerate(v_list):\n",
    "        nexti = (i + 1) % n\n",
    "        vel_diff = x - v_list[nexti]\n",
    "        if vel_diff > 0:  # signifficant note (1 -> 0)\n",
    "            mw_diff = mw[nexti] - mw[i]\n",
    "            sync_list[i] = vel_diff * mw_diff\n",
    "    return (sum(sync_list) + 15) / 30\n",
    "\n",
    "def v_syncopation_awareness(v_list):\n",
    "    if v_onset_count(v_list) == 0:\n",
    "        return 0\n",
    "    v_list = np.array(v_list)\n",
    "    # iterate over pattern and find if next velocity is smaller.\n",
    "    # that note should give a value as it is either a pulse reinforcement\n",
    "    # or a syncopation. the degree is given by the diference in velocity\n",
    "    mw = [5, 1, 2, 1, 3, 1, 2, 1, 4, 1, 2, 1, 3, 1, 2, 1]  # metrical weights\n",
    "    n = len(v_list)\n",
    "    sync_list = np.zeros(n)\n",
    "    for i, x in enumerate(v_list):\n",
    "        nexti = (i + 1) % n\n",
    "        vel_diff = x - v_list[nexti]\n",
    "        if vel_diff > 0:  # signifficant note (1 -> 0)\n",
    "            mw_diff = mw[nexti] - mw[i]\n",
    "            sync_list[i] = vel_diff * mw_diff\n",
    "    # take into account the listener awareness based on the part being played\n",
    "    sync_list[:4] = sync_list[:4] * 8\n",
    "    sync_list[4:8] = sync_list[4:8] * 1\n",
    "    sync_list[8:12] = sync_list[8:12] * 4\n",
    "    sync_list[12:16] = sync_list[12:16] * 2\n",
    "    return ((sum(sync_list) + 65)) / 115\n",
    "\n",
    "def v_evenness(v_list):\n",
    "    # how well distributed are the D onsets of a pattern\n",
    "    # if they are compared to a perfect D sided polygon\n",
    "    # input patterns are phase-corrected to start always at step 0\n",
    "    # i.e. if we have 4 onsets in a 16 step pattern, what is the distance of onsets\n",
    "    # o1, o2, o3, o4 to positions 0 4 8 and 12\n",
    "    # here we will use a simple algorithm that does not involve DFT computation\n",
    "    # evenness is well described in [Milne and Dean, 2016] but this implementation is much simpler\n",
    "    d = (np.array(v_list) > 0).sum()  # count onsets\n",
    "    if d <= 1:\n",
    "        return 0\n",
    "    iso_angle_16 = 2 * math.pi / 16  # angle of 1/16th\n",
    "    first_onset_step = [i for i, x in enumerate(v_list) if x != 0][0]\n",
    "    first_onset_angle = first_onset_step * iso_angle_16\n",
    "    iso_angle = 2 * math.pi / d\n",
    "    # ideal positions in radians\n",
    "    iso_pattern_radians = [x * iso_angle for x in range(d)]\n",
    "    # real positions in radians\n",
    "    pattern_radians = [i * iso_angle_16 for i, x in enumerate(v_list) if x != 0]\n",
    "    # sum distortion between ideal and real\n",
    "    cosines = [\n",
    "        abs(math.cos(x - pattern_radians[i] + first_onset_angle))\n",
    "        for i, x in enumerate(iso_pattern_radians)\n",
    "    ]\n",
    "    return sum(cosines) / d\n",
    "\n",
    "def v_balance(v_list):\n",
    "    # balance is described in [Milne and Herff, 2020] as:\n",
    "    # \"a quantification of the proximity of that rhythm's\n",
    "    # “centre of mass” (the mean position of the points)\n",
    "    # to the centre of the unit circle.\"\n",
    "    d = (np.array(v_list) > 0).sum()  # count onsets\n",
    "    if d <= 1:\n",
    "        return 0\n",
    "    center = np.array([0, 0])\n",
    "    iso_angle_16 = 2 * math.pi / 16\n",
    "    X = [math.cos(i * iso_angle_16) for i, x in enumerate(v_list) if x != 0]\n",
    "    Y = [math.sin(i * iso_angle_16) for i, x in enumerate(v_list) if x != 0]\n",
    "    matrix = np.array([X, Y])\n",
    "    matrix_sum = matrix.sum(axis=1)\n",
    "    magnitude = np.linalg.norm(matrix_sum - center) / d\n",
    "    return 1 - magnitude\n",
    "\n",
    "def v_syness(v_list):\n",
    "    # compare the syncopation and the number of onsets\n",
    "    # syness is higher when syncopation is high and density is small\n",
    "    # NOTE: no need to divide by v_density as syncopation has\n",
    "    # already taken velocity into account\n",
    "    onset_count = (np.array(v_list) > 0).sum()\n",
    "    if onset_count == 0:\n",
    "        return 0\n",
    "    res = (v_syncopation_awareness(v_list) / onset_count) / 0.6333333333333333\n",
    "    # in very few cases the result can be slightly above 1\n",
    "    if res > 1:\n",
    "        return 1\n",
    "    return res  # normalized for max syness\n",
    "\n",
    "descriptor_functions = {\n",
    "    \"onset_count\": v_onset_count,\n",
    "    \"start\": v_start,\n",
    "    \"center\": v_center,\n",
    "    \"syncopation\": v_syncopation,\n",
    "    \"syncopation_awareness\": v_syncopation_awareness,\n",
    "    \"evenness\": v_evenness,\n",
    "    \"balance\": v_balance,\n",
    "    \"syness\": v_syness,\n",
    "}\n",
    "\n",
    "def describe(v_list, descriptors_to_use):\n",
    "    computed_descriptors = {}\n",
    "    descriptors = [d for d in descriptors_to_use if '/' not in d]\n",
    "    for d in descriptors:\n",
    "        computed_descriptors[d] = descriptor_functions[d](v_list)\n",
    "    final_descriptors = [computed_descriptors[d] for d in descriptors]\n",
    "    descriptor_combinations = [d for d in descriptors_to_use if '/' in d]\n",
    "    for combination in descriptor_combinations:\n",
    "        d1, d2 = combination.split('/')\n",
    "        if d1 in computed_descriptors and d2 in computed_descriptors:\n",
    "            val1 = computed_descriptors[d1]\n",
    "            val2 = computed_descriptors[d2]\n",
    "            combined_value = val1 / val2 if val2 != 0 else 0\n",
    "            combined_value = min(max(combined_value, 0), 1)\n",
    "            final_descriptors.append(combined_value)\n",
    "        else:\n",
    "            final_descriptors.append(0)\n",
    "    scaled_descriptors = [int(d * 127) / 127 for d in final_descriptors]\n",
    "    return tuple(scaled_descriptors)\n",
    "\n",
    "def binary_combinations(steps):\n",
    "    # create all binary combinations\n",
    "    # given the number of states\n",
    "    # i.e. 2 = [(0,0), (1,0), (0,1), (1,1)]\n",
    "    combos = []\n",
    "    for event in range(2**steps):\n",
    "        ensemble = []\n",
    "        for i, _ in enumerate(range(steps)):\n",
    "            t = ((event * 2) // (2 ** (i + 1))) % 2\n",
    "            ensemble.append(t)\n",
    "        combos.append(tuple(ensemble))\n",
    "    return combos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generate dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(selected_descriptors, log=True):\n",
    "    if log: print(f\"Selected descriptors: {selected_descriptors}\")\n",
    "\n",
    "    n_descriptors = len(selected_descriptors)\n",
    "    all_patterns = binary_combinations(16)\n",
    "    all_descriptors = np.zeros([len(all_patterns), n_descriptors])\n",
    "    for i, p in enumerate(all_patterns):\n",
    "        all_descriptors[i] = describe(p, selected_descriptors)\n",
    "    df = pd.DataFrame(all_descriptors)\n",
    "    df.columns = selected_descriptors\n",
    "    # save the df to a .csv file\n",
    "    df.to_csv(\"descriptors.csv\", index=True)\n",
    "\n",
    "    unique, inverse, counts = np.unique(all_descriptors, axis=0, return_inverse=True, return_counts=True)\n",
    "    if log: print(f\"{len(all_descriptors) - len(unique)} are repeated\")\n",
    "    if log: print(f\"{len(unique) / len(all_descriptors)}% are uniquely identified\")\n",
    "    # unique: redcued list\n",
    "    # inverse: reconstruction using unique as index (len(inverse) == len(all_descriptors))\n",
    "    # counts: counts of the unique\n",
    "    # print patterns that have the same descriptor values thus have the same inverse value.\n",
    "    # indexes of patterns that have a similar descirptor set as other(s)\n",
    "    repeated = [i for i, x in enumerate(counts) if x > 1]\n",
    "    # find the indexes of these indexes in the inverse list\n",
    "    for x in repeated:\n",
    "        rep_index = np.where(inverse == x)[0]\n",
    "        for xx in rep_index:\n",
    "            pass\n",
    "    return n_descriptors, all_descriptors, all_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, n_descriptors, all_descriptors, all_patterns, descriptor_labels, log=True):\n",
    "    X = torch.tensor(all_descriptors, dtype=torch.float32).to(\"cpu\")\n",
    "    y = torch.tensor(all_patterns, dtype=torch.float32).to(\"cpu\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "\n",
    "    class Multiclass(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.hidden = nn.Linear(n_descriptors, 16)\n",
    "            self.act = nn.ReLU()\n",
    "            self.hidden1 = nn.Linear(16, 32)\n",
    "            self.act1 = nn.ReLU()\n",
    "            self.hidden2 = nn.Linear(32, 64)\n",
    "            self.act2 = nn.ReLU()\n",
    "            self.hidden3 = nn.Linear(64, 32)\n",
    "            self.act3 = nn.ReLU()\n",
    "            self.output = nn.Linear(32, 16)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.act(self.hidden(x))\n",
    "            x = self.act1(self.hidden1(x))\n",
    "            x = self.act2(self.hidden2(x))\n",
    "            x = self.act3(self.hidden3(x))\n",
    "            x = self.output(x)\n",
    "            return x\n",
    "\n",
    "    model = Multiclass()\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    n_epochs = 200\n",
    "    batch_size = 32\n",
    "    batches_per_epoch = len(X_train) // batch_size\n",
    "\n",
    "    best_acc = -np.inf\n",
    "    best_weights = None\n",
    "    train_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    test_loss_hist = []\n",
    "    test_acc_hist = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = []\n",
    "        epoch_acc = []\n",
    "\n",
    "        for i in range(batches_per_epoch):\n",
    "            start = i * batch_size\n",
    "            X_batch = X_train[start : start + batch_size]\n",
    "            y_batch = y_train[start : start + batch_size]\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            acc = (torch.argmax(y_pred, 1) == torch.argmax(y_batch, 1)).float().mean()\n",
    "\n",
    "            epoch_loss.append(float(loss))\n",
    "            epoch_acc.append(float(acc))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        y_pred = model(X_test)\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        predicted_patterns = torch.where(y_pred > 0.5, 1, 0)  # threshold > 0.5\n",
    "\n",
    "        # Compute descriptors for predicted patterns batch-wise\n",
    "        predicted_descriptors = np.array([describe(pattern.tolist(), descriptor_labels) for pattern in predicted_patterns])\n",
    "\n",
    "        # Retrieve actual descriptors from the test set\n",
    "        provided_descriptors = X_test.cpu().numpy()\n",
    "\n",
    "        y_pred = model(X_test)\n",
    "        ce = loss_fn(y_pred, y_test)\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        predicted_pattern = torch.where(y_pred > 0.5, 1, 0)  # threshold > 0.5\n",
    "\n",
    "        # Compute mean absolute error (or another metric)\n",
    "        acc = 1 - np.mean(np.abs(predicted_descriptors - provided_descriptors).sum(axis=1) / n_descriptors)\n",
    "\n",
    "        ce = float(loss_fn(y_pred, y_test))\n",
    "\n",
    "        train_loss_hist.append(float(loss))\n",
    "        train_acc_hist.append(float(acc))\n",
    "        test_loss_hist.append(ce)\n",
    "        test_acc_hist.append(acc)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == n_epochs - 1:\n",
    "            if log: print(f\"Epoch {epoch} validation: Cross-entropy={ce:.2f}, Accuracy={acc*100:.1f}%\")\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "\n",
    "    torch.save(best_weights, \"./models/\" + model_name + \".pth\")\n",
    "\n",
    "    return best_acc, train_loss_hist, train_acc_hist, test_loss_hist, test_acc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onset_count\n",
    "# start\n",
    "# center\n",
    "# syncopation\n",
    "# syncopation_awareness\n",
    "# evenness\n",
    "# balance\n",
    "# syness\n",
    "\n",
    "d = [\"onset_count\", \"start\", \"center\", \"syncopation\", \"balance\"]\n",
    "\n",
    "n_descriptors, all_descriptors, all_patterns = generate_dataset(d, log=True)\n",
    "model_name = \"d2p_d_err\"\n",
    "best_acc, train_loss_hist, train_acc_hist, test_loss_hist, test_acc_hist = train_model(model_name, n_descriptors, all_descriptors, all_patterns, d, log=True)\n",
    "print(f\"Best accuracy: ({best_acc*100:.2f}%)\")\n",
    "print(f\"Model saved at: ./models/{model_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "class Multiclass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(n_descriptors, 16)\n",
    "        self.act = nn.ReLU()\n",
    "        self.hidden1 = nn.Linear(16, 32)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(32, 64)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.hidden3 = nn.Linear(64, 32)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.hidden(x))\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act3(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = Multiclass()\n",
    "model.load_state_dict(torch.load(f\"./models/{model_name}.pth\"))\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# Generate a random set of descriptors\n",
    "random_descriptors = torch.rand((1, n_descriptors), dtype=torch.float32).to(\"cpu\")\n",
    "\n",
    "# Predict using the model\n",
    "with torch.no_grad():\n",
    "    prediction = model(random_descriptors)\n",
    "    prediction = torch.sigmoid(prediction)\n",
    "    # predicted_pattern = torch.where(prediction > 0.5, 1, 0)\n",
    "    predicted_pattern = prediction\n",
    "\n",
    "print(d)\n",
    "print(f\"Random descriptors: {random_descriptors.tolist()[0]}\")\n",
    "print(f\"Predicted pattern: {predicted_pattern.tolist()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, random_descriptors, f\"./models/{model_name}.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(f\"./models/{model_name}.onnx\")\n",
    "for input in onnx_model.graph.input:\n",
    "    print(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
