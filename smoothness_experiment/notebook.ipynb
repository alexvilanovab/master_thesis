{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothness experiment\n",
    "\n",
    "In this notebook, we want to explore how smooth is the interaction with the descriptors to pattern models we have been working on. We will define a set of movements within the descriptor space and we will analyze how smooth the transitions are. We will also provide a Pure Data patch that will allow us to listen to the movements.\n",
    "\n",
    "---\n",
    "\n",
    "What we currently have:\n",
    "- We have defined the following 5 descriptors:\n",
    "    - `onset_count`\n",
    "    - `start`\n",
    "    - `center`\n",
    "    - `syncopation`\n",
    "    - `balance`\n",
    "- We have trained a model that can generate 16 step patterns given values for the 5 descriptors.\n",
    "- We have a Pure Data patch to interactively explore the descriptor space and listen to the generated patterns.\n",
    "\n",
    "---\n",
    "\n",
    "The experiment will be as follows:\n",
    "- We will generate 3000 movements, each movement will consist of 32 interpolations between a point A and a point B.\n",
    "- Point A will be randomly generated inside the 5 dimensional descriptor space.\n",
    "- Point B will be at a certain distance from point A, also randomly generated and within the 5 dimensional descriptor space.\n",
    "- For each movement, we will only consider changing two descriptors at a time (if each descriptor is a knob, it is natural to only move 2 knobs at a time)\n",
    "- We define 3 different type of movements, each corresponding to a different distance between the two points.\n",
    "    - `small` -> `sqrt(2) * 0.25`\n",
    "    - `medium` -> `sqrt(2) * 0.5`\n",
    "    - `large` -> `sqrt(2) * 0.75`\n",
    "- For each set of descriptors within a movement (32 interpolations), we will run the model and compute the corresponding 16 step pattern.\n",
    "- At this point, we will analyze 2 things in parallel:\n",
    "    - KL divergence between the distribution of the two patterns in each interpolation step.\n",
    "    - Generate descriptors given the two patterns in each interpolation step and compute the euclidean distance between the 2 sets of descriptors.\n",
    "- For each two analyses, we will make a plot where the X axis will be the interpolation step and the Y axis will be the KL divergence or the euclidean distance respectively.\n",
    "- We will also make a boxplot for each interpolation step, showing the distribution of the KL divergence or the euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ['onset_count', 'start', 'center', 'syncopation', 'balance']\n",
    "n_iterations = 3000  # number of movements\n",
    "n_interpolation = 32  # number of steps\n",
    "n_descriptors = 5\n",
    "max_distance = np.sqrt(2)\n",
    "movement_types = {\n",
    "    \"small\": max_distance * 0.25,\n",
    "    \"medium\": max_distance * 0.5,\n",
    "    \"large\": max_distance * 0.75,\n",
    "}\n",
    "models = [\"d2p_p_err\", \"d2p_d_err\"]\n",
    "model_paths = [f\"../d2p_model/models/{m}.pth\" for m in models]\n",
    "n_per_category = n_iterations // 3  # ensure balanced distribution of movement types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate experiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_point():\n",
    "    \"\"\"Generates a random point in the 5D space with values between 0 and 1.\"\"\"\n",
    "    A = np.random.rand(n_descriptors)\n",
    "    if A[1] > A[2]:\n",
    "        A[1], A[2] = A[2], A[1]  # swap start (A[1]) and center (A[2]) if start is greater than center\n",
    "    return A\n",
    "\n",
    "def generate_b_point(A, descriptors_moved, distance):\n",
    "    \"\"\"Generates point B at a specific distance from A in the space defined by descriptors_moved.\n",
    "    If the generated point goes out of bounds, a new direction is generated until a valid point is found.\n",
    "    If a valid point is not found after several attempts, None is returned.\"\"\"\n",
    "    max_attempts = 100\n",
    "    for _ in range(max_attempts):\n",
    "        direction = np.random.randn(2)\n",
    "        direction /= np.linalg.norm(direction)  # normalize direction\n",
    "        delta = direction * distance\n",
    "        B = A.copy()\n",
    "        B[descriptors_moved] += delta\n",
    "\n",
    "        if np.all((B >= 0) & (B <= 1)):\n",
    "            return B  # make sure all elements of B are within bounds [0,1]\n",
    "    \n",
    "    return None  # if a valid point was not found after max_attempts, return None\n",
    "\n",
    "def compute_euclidean_distance(p1, p2):\n",
    "    \"\"\"Compute Euclidean distance between two points.\"\"\"\n",
    "    return np.linalg.norm(p1 - p2)\n",
    "\n",
    "def run_experiment():\n",
    "    \"\"\"Runs the experiment and returns a DataFrame with the results.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    movement_categories = [\"small\"] * n_per_category + [\"medium\"] * n_per_category + [\"large\"] * n_per_category\n",
    "    np.random.shuffle(movement_categories)  # shuffle the order to randomize sampling\n",
    "    \n",
    "    for movement_type in movement_categories:\n",
    "        B = None\n",
    "        while B is None:\n",
    "            distance = movement_types[movement_type]\n",
    "            descriptors_moved = np.random.choice(n_descriptors, 2, replace=False)\n",
    "            \n",
    "            A = generate_random_point()\n",
    "            B = generate_b_point(A, descriptors_moved, distance)\n",
    "            if B is not None and B[1] > B[2]:\n",
    "                B = None  # if the center always >= start constraint is not met, generate a new B\n",
    "\n",
    "        # interpolate between A and B\n",
    "        steps = np.linspace(0, 1, n_interpolation)[:, np.newaxis]\n",
    "        trajectory = (1 - steps) * A + steps * B\n",
    "        \n",
    "        results.append({\n",
    "            \"distance_category\": movement_type,\n",
    "            \"descriptors_moved\": descriptors_moved.tolist(),\n",
    "            # \"A\": A.tolist(),\n",
    "            # \"B\": B.tolist(),\n",
    "            \"trajectory\": trajectory.tolist(),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# run and save results\n",
    "df_results = run_experiment()\n",
    "df_results.to_csv('movements.csv', index=False)\n",
    "\n",
    "# make sure everything is correct\n",
    "df_results = pd.read_csv('movements.csv')\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute all patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiclass(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(input_dim, 16)\n",
    "        self.act = nn.ReLU()\n",
    "        self.hidden1 = nn.Linear(16, 32)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(32, 64)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.hidden3 = nn.Linear(64, 32)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.hidden(x))\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act3(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Define model-architecture mapping\n",
    "model_architectures = {\n",
    "    \"d2p_p_err\": Multiclass,\n",
    "    \"d2p_d_err\": Multiclass,\n",
    "    # Add different architectures if needed\n",
    "}\n",
    "\n",
    "def load_model(model_name, input_dim):\n",
    "    model_class = model_architectures[model_name]\n",
    "    model = model_class(input_dim)\n",
    "    model.to(\"cpu\")\n",
    "    model.load_state_dict(torch.load(f\"../d2p_model/models/{model_name}.pth\", map_location=\"cpu\"))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "models = {name: load_model(name, n_descriptors) for name in model_architectures}\n",
    "\n",
    "def compute_pattern(descriptors, model_name):\n",
    "    model = models[model_name]\n",
    "    descriptors_tensor = torch.tensor(np.array([descriptors]), dtype=torch.float32).to(\"cpu\")\n",
    "    with torch.no_grad():\n",
    "        prediction = model(descriptors_tensor)\n",
    "        prediction = torch.sigmoid(prediction)\n",
    "    prediction = [int(value * 127) for value in prediction.tolist()[0]]\n",
    "    return prediction\n",
    "\n",
    "df_results[[f\"patterns_{model_name}\" for model_name in models]] = df_results.apply(\n",
    "    lambda row: pd.Series({f\"patterns_{model_name}\": [compute_pattern(np.array(step, dtype=np.float32), model_name) for step in eval(row['trajectory'])] for model_name in models}),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert all patterns to descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_density(v_list):\n",
    "  if v_list == []*len(v_list): #check if empty\n",
    "    return 0\n",
    "  v_list = np.array(v_list) #convert to numpy\n",
    "\n",
    "  # normalize if necessary\n",
    "  if np.greater(np.max(v_list), 1): #if we have numbers greater than 1\n",
    "    v_list = v_list/127\n",
    "\n",
    "  return np.mean(v_list)\n",
    "\n",
    "def v_onset_count(v_list):\n",
    "  if v_list == []*len(v_list): #check if empty\n",
    "    return 0\n",
    "  return (np.array(v_list) > 0).sum()\n",
    "\n",
    "def v_syncopation(v_list):\n",
    "  v_list = np.array(v_list) #convert to numpy\n",
    "  if np.count_nonzero(v_list ==0) == len(v_list) or len(v_list) == 0: #check if empty\n",
    "    return 0, [0]*len(v_list)\n",
    "\n",
    "  # normalize if necessary\n",
    "  if np.greater(np.max(v_list), 1): #if we have numbers greater than 1\n",
    "    v_list = v_list/127\n",
    "\n",
    "  # iterate over pattern and find if next velocity is smaller.\n",
    "  # that note should give a value as it is either a pulse reinforcement\n",
    "  # or a syncopation. The degree is given by the diference in velocity.\n",
    "  mw = [5,1,2,1,3,1,2,1,4,1,2,1,3,1,2,1] # metrical weights\n",
    "  sync_list = np.zeros([len(v_list)])\n",
    "  for i,x in enumerate (v_list):\n",
    "    vel_diff = x-v_list[(i+1)%len(v_list)]\n",
    "    if vel_diff > 0 and mw[i]<mw[(i+1)%len(v_list)]: #signifficant note & syncopation\n",
    "      mw_diff = mw[(i+1)%len(v_list)]-mw[i]\n",
    "      sync_list [i] = vel_diff * mw_diff\n",
    "  return sum (sync_list)/15, sync_list/4 # max sync at any step is 4\n",
    "\n",
    "def v_metricality (v_list):\n",
    "  # How metrical is the current vlist\n",
    "  # only observe notes on higher metrical levels than next step silence\n",
    "  v_list = np.array(v_list) #convert to numpy\n",
    "  if np.count_nonzero(v_list ==0) == len(v_list) or len(v_list) == 0: #check if empty\n",
    "    return 0, [0]*len(v_list)\n",
    "  # normalize if necessary\n",
    "  if np.greater(np.max(v_list), 1): #if we have numbers greater than 1\n",
    "    v_list = v_list/127\n",
    "  mw = [5,1,2,1,3,1,2,1,4,1,2,1,3,1,2,1] # metrical weights\n",
    "  metricality_list = np.zeros([len(v_list)])\n",
    "  for i,x in enumerate(v_list):\n",
    "    vel_diff = x-v_list[(i+1)%len(v_list)]\n",
    "    if vel_diff > 0 and mw[i]>mw[(i+1)%len(v_list)]: #signifficant note & pulse reinforcement\n",
    "      mw_diff = mw[i] - mw[(i+1)%len(v_list)]\n",
    "      metricality_list[i] = vel_diff * mw_diff\n",
    "\n",
    "  return sum (metricality_list)/15, metricality_list/4 # max metr at any step is 4\n",
    "\n",
    "def v_syness(v_list):\n",
    "  # compare the syncopation and the number of onsets\n",
    "  # syness is higher when syncopation is high and density is small\n",
    "  # NOTE: no need to divide by v_density as syncopation has\n",
    "  # already taken velocity into account\n",
    "  if v_list == [] or v_list == [0]*len(v_list): #check if empty\n",
    "    return 0\n",
    "  return (v_syncopation(v_list)[0] / v_onset_count(v_list)) / 0.26666666666666666 # normalized for max syness\n",
    "\n",
    "def v_meterness(v_list):\n",
    "  # compare the metricality and the number of onsets\n",
    "  # meterness is higher when metricality is high and density is small\n",
    "  # NOTE: no need to normalize by v_denisty as metricality has been normalized\n",
    "  if v_list == [] or v_list == [0]*len(v_list): #check if empty\n",
    "    return 0\n",
    "  return v_metricality(v_list)[0]/v_onset_count(v_list)/ 0.26666666666666666 #normalize for max meterness\n",
    "\n",
    "def v_evenness(v_list):\n",
    "    # how well distributed are the D onsets of a pattern\n",
    "    # if they are compared to a perfect D sided polygon\n",
    "    # input patterns are phase-corrected to start always at step 0\n",
    "    # i.e. if we have 4 onsets in a 16 step pattern, what is the distance of onsets\n",
    "    # o1, o2, o3, o4 to positions 0 4 8 and 12\n",
    "    # here we will use a simple algorithm that does not involve DFT computation\n",
    "    # evenness is well described in [Milne and Dean, 2016] but this implementation is much simpler\n",
    "\n",
    "    d = v_onset_count(v_list)\n",
    "    if d <=1:\n",
    "      return 0\n",
    "\n",
    "    iso_angle_16 = 2 * math.pi / 16 # angle of 1/16th\n",
    "    first_onset_step = [i for i, x in enumerate(v_list) if x != 0][0]\n",
    "    first_onset_angle = first_onset_step * iso_angle_16\n",
    "    iso_angle = 2 * math.pi / d\n",
    "    # ideal angles\n",
    "    iso_pattern_radians = [x * iso_angle for x in range(d)]\n",
    "    # actual distances\n",
    "    pattern_radians = [i * iso_angle_16 for i, x in enumerate(v_list) if x != 0]\n",
    "    cosines = [\n",
    "        abs(math.cos(x - pattern_radians[i] + first_onset_angle))\n",
    "        for i, x in enumerate(iso_pattern_radians)\n",
    "    ]\n",
    "    return sum(cosines) / d\n",
    "\n",
    "def v_balance(v_list):\n",
    "    # balance is described in [Milne and Herff, 2020] as:\n",
    "    # \"a quantification of the proximity of that rhythm's\n",
    "    # “centre of mass” (the mean position of the points)\n",
    "    # to the centre of the unit circle.\"\n",
    "    d = v_onset_count(v_list)\n",
    "    if d <= 1:\n",
    "        return 0\n",
    "\n",
    "    center = np.array([0, 0])\n",
    "    iso_angle_16 = 2 * math.pi / 16\n",
    "    X = [math.cos(i * iso_angle_16) for i, x in enumerate(v_list) if x != 0]\n",
    "    Y = [math.sin(i * iso_angle_16) for i, x in enumerate(v_list) if x != 0]\n",
    "    matrix = np.array([X, Y])\n",
    "    matrix_sum = matrix.sum(axis=1)\n",
    "    magnitude = np.linalg.norm(matrix_sum - center) / d\n",
    "    return 1 - magnitude\n",
    "\n",
    "def v_start(v_list):\n",
    "  # output the starting step of the pattern\n",
    "  if v_onset_count(v_list) == 0:\n",
    "    return 0\n",
    "  s = 0\n",
    "  while v_list[s] == 0:\n",
    "    s += 1\n",
    "  return s / (len(v_list))\n",
    "\n",
    "prediction_d = [\"density\", \"onset_count\", \"syncopation\", \"metricality\", \"syness\", \"meterness\", \"evenness\", \"balance\", \"start\"]\n",
    "\n",
    "def describe_pattern(pattern):\n",
    "    return [\n",
    "        v_density(pattern),\n",
    "        v_onset_count(pattern),\n",
    "        v_syncopation(pattern)[0],\n",
    "        v_metricality(pattern)[0],\n",
    "        v_syness(pattern),\n",
    "        v_meterness(pattern),\n",
    "        v_evenness(pattern),\n",
    "        v_balance(pattern),\n",
    "        v_start(pattern),\n",
    "    ]\n",
    "\n",
    "for model_name in models:\n",
    "    pattern_col = f\"patterns_{model_name}\"\n",
    "    descriptor_col = f\"descriptors_{model_name}\"\n",
    "    df_results[descriptor_col] = df_results[pattern_col].apply(lambda patterns: [describe_pattern(pattern) for pattern in patterns])\n",
    "\n",
    "df_results.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Execute KL divergence and euclidean distance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute KL divergence (avoiding zero issues)\n",
    "def kl_divergence(p, q):\n",
    "    p = np.array(p) + 1e-10  # Small offset to avoid division by zero\n",
    "    q = np.array(q) + 1e-10\n",
    "    return entropy(p, q)\n",
    "\n",
    "# Function to compute Euclidean distance\n",
    "def euclidean_distance(d1, d2):\n",
    "    return np.linalg.norm(np.array(d1) - np.array(d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "movement_types = [\"small\", \"medium\", \"large\"]\n",
    "kl_means = {model: {} for model in models}\n",
    "euc_means = {model: {} for model in models}\n",
    "\n",
    "kl_palette = [\"royalblue\", \"darkorange\", \"seagreen\"]\n",
    "euc_palette = [\"crimson\", \"purple\", \"saddlebrown\"]\n",
    "\n",
    "for model in models:\n",
    "    pattern_col = f\"patterns_{model}\"\n",
    "    descriptor_col = f\"descriptors_{model}\"\n",
    "\n",
    "    for movement_type in movement_types:\n",
    "        df = df_results[df_results[\"distance_category\"] == movement_type]\n",
    "        kl_all, euc_all = [], []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            pattern_steps = row[pattern_col]\n",
    "            descriptor_steps = row[descriptor_col]\n",
    "            \n",
    "            if len(pattern_steps) == n_interpolation and len(descriptor_steps) == n_interpolation:\n",
    "                kl_divs = [kl_divergence(pattern_steps[i], pattern_steps[i+1]) for i in range(n_interpolation - 1)]\n",
    "                euc_dists = [euclidean_distance(descriptor_steps[i], descriptor_steps[i+1]) for i in range(n_interpolation - 1)]\n",
    "                \n",
    "                kl_all.append(np.cumsum(kl_divs))  # Accumulate KL divergence\n",
    "                euc_all.append(np.cumsum(euc_dists))  # Accumulate Euc. distance\n",
    "            \n",
    "        kl_means[model][movement_type] = np.mean(kl_all, axis=0)\n",
    "        euc_means[model][movement_type] = np.mean(euc_all, axis=0)\n",
    "\n",
    "# Create dataframes for plotting\n",
    "kl_dfs = {model: pd.DataFrame(kl_means[model]) for model in models}\n",
    "euc_dfs = {model: pd.DataFrame(euc_means[model]) for model in models}\n",
    "\n",
    "for model in models:\n",
    "    kl_dfs[model][\"Interpolation Step\"] = np.arange(n_interpolation - 1)\n",
    "    euc_dfs[model][\"Interpolation Step\"] = np.arange(n_interpolation - 1)\n",
    "\n",
    "# Determine global y-axis limits\n",
    "max_kl = max(df.drop(columns=\"Interpolation Step\").max().max() for df in kl_dfs.values())\n",
    "max_euc = max(df.drop(columns=\"Interpolation Step\").max().max() for df in euc_dfs.values())\n",
    "\n",
    "# Create subplots with two columns and four rows\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16/1.5, 18/1.5))  # 4 rows, 2 columns\n",
    "\n",
    "model_name = {\n",
    "    \"d2p_p_err\": \"Pattern-based eval.\",\n",
    "    \"d2p_d_err\": \"Descriptor-based eval.\",\n",
    "}\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    # KL Div. Line Plot (First Column)\n",
    "    kl_melted = kl_dfs[model].melt(id_vars=\"Interpolation Step\", var_name=\"Movement Type\", value_name=\"KL Div.\")\n",
    "    sns.lineplot(data=kl_melted, x=\"Interpolation Step\", y=\"KL Div.\", hue=\"Movement Type\", palette=kl_palette, marker=\"o\", ax=axes[0, i])\n",
    "    axes[0, i].set_title(f\"Acc. KL Div. ({model_name[model]})\", fontsize=14)\n",
    "    axes[0, i].set_xlabel(\"Interpolation Step\")\n",
    "    axes[0, i].set_ylabel(\"Acc. KL Div.\")\n",
    "    axes[0, i].grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    axes[0, i].legend(fontsize=8, loc='upper left')\n",
    "    axes[0, i].set_ylim(0, max_kl)  # Set common y-axis limit\n",
    "\n",
    "    # KL Div. Box Plot (Second Column)\n",
    "    sns.boxplot(data=kl_dfs[model].drop(columns=\"Interpolation Step\"), palette=kl_palette, ax=axes[1, i])\n",
    "    axes[1, i].set_xlabel(\"Movement Type\")\n",
    "    axes[1, i].set_ylabel(\"Acc. KL Div.\")\n",
    "    axes[1, i].set_title(f\"KL Divergence Dist. ({model_name[model]})\", fontsize=14)\n",
    "    axes[1, i].grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    axes[1, i].set_ylim(0, max_kl)  # Set common y-axis limit\n",
    "\n",
    "    # Euc. Dist. Line Plot (Third Column)\n",
    "    euc_melted = euc_dfs[model].melt(id_vars=\"Interpolation Step\", var_name=\"Movement Type\", value_name=\"Euc. Dist.\")\n",
    "    sns.lineplot(data=euc_melted, x=\"Interpolation Step\", y=\"Euc. Dist.\", hue=\"Movement Type\", palette=euc_palette, marker=\"o\", ax=axes[2, i])\n",
    "    axes[2, i].set_title(f\"Acc. Euc. Dist. ({model_name[model]})\", fontsize=14)\n",
    "    axes[2, i].set_xlabel(\"Interpolation Step\")\n",
    "    axes[2, i].set_ylabel(\"Acc. Euc. Dist.\")\n",
    "    axes[2, i].grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    axes[2, i].legend(fontsize=8, loc='upper left')\n",
    "    axes[2, i].set_ylim(0, max_euc)  # Set common y-axis limit\n",
    "\n",
    "    # Euc. Dist. Box Plot (Fourth Column)\n",
    "    sns.boxplot(data=euc_dfs[model].drop(columns=\"Interpolation Step\"), palette=euc_palette, ax=axes[3, i])\n",
    "    axes[3, i].set_xlabel(\"Movement Type\")\n",
    "    axes[3, i].set_ylabel(\"Acc. Euc. Dist.\")\n",
    "    axes[3, i].set_title(f\"Euc. Dist. ({model_name[model]})\", fontsize=14)\n",
    "    axes[3, i].grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    axes[3, i].set_ylim(0, max_euc)  # Set common y-axis limit\n",
    "\n",
    "# fig.suptitle(\"Comparison of Acc. KL Div. and Euc. Dist. Across Models and Movement Types\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. ANOVA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Loop through the models\n",
    "for model in models:\n",
    "    # Get the relevant columns for each model's KL Divergence and Euclidean Distance\n",
    "    kl_df_model = kl_dfs[model]\n",
    "    euc_df_model = euc_dfs[model]\n",
    "\n",
    "    # Perform ANOVA for KL Divergence for each model\n",
    "    kl_anova_p = f_oneway(kl_df_model[\"small\"], kl_df_model[\"medium\"], kl_df_model[\"large\"]).pvalue\n",
    "    print(f\"ANOVA p-value for KL Divergence ({model}): {kl_anova_p}\")\n",
    "\n",
    "    # Perform ANOVA for Euclidean Distance for each model\n",
    "    euc_anova_p = f_oneway(euc_df_model[\"small\"], euc_df_model[\"medium\"], euc_df_model[\"large\"]).pvalue\n",
    "    print(f\"ANOVA p-value for Euclidean Distance ({model}): {euc_anova_p}\")\n",
    "\n",
    "    # If ANOVA for KL Divergence is significant, perform Tukey's HSD test\n",
    "    if kl_anova_p < 0.05:\n",
    "        kl_melted = kl_df_model.melt(id_vars=\"Interpolation Step\", var_name=\"Movement Type\", value_name=\"KL Divergence\")\n",
    "        kl_tukey = pairwise_tukeyhsd(kl_melted[\"KL Divergence\"], kl_melted[\"Movement Type\"], alpha=0.05)\n",
    "        print(f\"Tukey's HSD test for KL Divergence ({model}):\")\n",
    "        print(kl_tukey)\n",
    "\n",
    "    # If ANOVA for Euclidean Distance is significant, perform Tukey's HSD test\n",
    "    if euc_anova_p < 0.05:\n",
    "        euc_melted = euc_df_model.melt(id_vars=\"Interpolation Step\", var_name=\"Movement Type\", value_name=\"Euclidean Distance\")\n",
    "        euc_tukey = pairwise_tukeyhsd(euc_melted[\"Euclidean Distance\"], euc_melted[\"Movement Type\"], alpha=0.05)\n",
    "        print(f\"Tukey's HSD test for Euclidean Distance ({model}):\")\n",
    "        print(euc_tukey)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Find outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the maximum KL divergence and Euclidean distance for each movement type\n",
    "# max_kl_divergence = df_results['patterns'].apply(lambda patterns: max([kl_divergence(patterns[i], patterns[i+1]) for i in range(n_interpolation - 1)]))\n",
    "# max_euclidean_distance = df_results['descriptors'].apply(lambda descriptors: max([euclidean_distance(descriptors[i], descriptors[i+1]) for i in range(n_interpolation - 1)]))\n",
    "\n",
    "# # Add these maximum values to the dataframe\n",
    "# df_results['max_kl_divergence'] = max_kl_divergence\n",
    "# df_results['max_euclidean_distance'] = max_euclidean_distance\n",
    "\n",
    "# # Find the rows with the maximum KL divergence and Euclidean distance\n",
    "# max_kl_row = df_results.loc[df_results['max_kl_divergence'].idxmax()]\n",
    "# max_euclidean_row = df_results.loc[df_results['max_euclidean_distance'].idxmax()]\n",
    "\n",
    "# print(\"Row with maximum KL divergence:\")\n",
    "# print(max_kl_row)\n",
    "\n",
    "# print(\"\\nRow with maximum Euclidean distance:\")\n",
    "# print(max_euclidean_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
